---
categories: 强化学习
---

在$REINFORCE$算法中，每次需要根据一个策略采集一条完整的轨迹,并计算这条轨迹上的回报。这种采样方式的方差比较大，学习效率也比较低。我们可以借鉴时序差分学习的思想，使用动态规划方法来提高采样的效率，即从状态开始$s$的总回报可以通过当前动作的即时奖励$r(s,a,s^{\prime})$和下一个状态$s^{\prime}$的值函数来近似估计。

演员评论员算法（Actor-Critic Algorithm）既不属于基于值函数的学习方法，也不属于基于策略函数的学习方法，它是结合**策略梯度**和**时序差分学习**的强化学习方法。

# 基本思想

## 演员（Actor）

演员（Actor）是指策略函数$\pi_{\theta}(s, a)​$，即学习一个策略来得到尽量高的回报。

## 评论员（Critic）

评论员（Critic）是指值函数$V_{\phi}(s)​$，对当前策略的值函数进行估计，即评估Actor的好坏。借助于值函数，Actor-Critic算法可以进行单步更新参数，不需要等到回合结束才进行更新。

**在该算法中，策略函数$\pi_{\theta}(s, a)$和值函数$V_{\phi}(s)​$都是待学习的函数，需要在训练中同时学习。**

## 训练过程

假设从时刻 t 开始的回报

