---
categories: 论文解读
---

# LP-TCP/RL-TCP

> 该论文的全称为“Improving TCP Congestion Control with Machine Intelligence”，收录于Sigcomm 2018。

## 摘要

该论文提出了两种拥塞控制机制，都是数据驱动的，它们分别是：

- LP-TCP：Loss Predictor (LP) based TCP CC，基于丢包预测的拥塞控制。
- RL-TCP：Reinforcement Learning (RL) based TCP CC，基于强化学习的拥塞控制。

作者在NS2平台上实现了这两种算法，并将其对传统算法进行了比较，发现这两个算法在**延迟**和**吞吐率**上达到了更好的均衡。

## 前置概念

### 拥塞控制算法三核心阶段

- 慢启动阶段（Slow Start）。
- 拥塞避免阶段（Congestion Avoidance）。
- 快速恢复（Fast Recovery）。

这三个阶段是大多数拥塞控制算法工作的基石。

## LP-TCP

其基本思想是使用**监督学习**，并将其置于TCP拥塞控制算法中去预测从而降低丢包率。

### 工作原理

1. 对于**感知引擎**的输入：接收到的ACK。
2. 对于**学习器**的输入：拥塞窗口的大小、拥塞窗口中目前包的顺序、EWMA、时间序列、最小ACK间隔时间，最小RTT。
3. 对于**行动器**的输入：对于将要发的包是否会**丢失的可能性**。

整个LPTCP设计的拓扑结构如下：

![](../img\LPTCP.png)

### State

为了反映当前网络的拥塞程度，感知引擎的输出是一个**长度为55的向量**。通过监督学习学习到当一个包即将发送时其丢失的概率，如果该概率低于一个瓶颈$th$，那么就发送，否则不进行发送。

注意，**决策发生在当一个包进行发送时，而非发生在接收到一个ACK时**。

### 缺点

当一个网络中的拓扑和参数改变时，需要学习一个新的LP-TCP。

## RL-TCP

### 工作原理

1. 对于**感知引擎**的输入：接收到的ACK。
2. 对于**学习器**的输入：状态向量和回报$r$。
3. 对于**执行器**的输入：**状态动作值**函数值。

当一个**新的ACK接收到**时，感知引擎感知当前网络的状态，并将其抽象化为一个长度为5维的向量，回报值$r$也通过感知引擎得到，$r$基于效用函数$U$的改变。

效用函数$U$是一个基于**吞吐量**$tp$、**延迟**$d=RTT-RTT_{min}$和丢包率$p$的函数，其具体定义如下：


$$
U=\log \left(\frac{t p}{B}\right)-\delta_{1} \log (d)+\delta_{2} \log (1-p)
$$


$B$代表网络的带宽，$\delta_{1}$和$\delta_{2}$代表偏置函数，第一项表示吞吐量越大越好，第二项表示延迟越低越好，第三项表示丢包率越小越好。

#### 学习算法

为了学习状态动作值函数$Q(s,a)$，采用了SARSA算法，全称为